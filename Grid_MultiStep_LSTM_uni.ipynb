{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Grid-MultiStep LSTM uni.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "84te-Mao0ynq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Odt36WZz1EX6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from math import sqrt\n",
        "from numpy import array\n",
        "from numpy import mean\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "from pandas import read_csv\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "dataset = read_csv(\"data_to_model.csv\", infer_datetime_format=True, parse_dates=['week_start_date'], index_col=['week_start_date'])\n",
        "\n",
        "qt_loc_data = dataset.drop(['year', 'week'], axis=1)\n",
        "\n",
        "loc_1_data = qt_loc_data.loc[qt_loc_data['location'] == 1].drop(['location'], axis=1)\n",
        "loc_2_data = qt_loc_data.loc[qt_loc_data['location'] == 2].drop(['location'], axis=1)\n",
        "loc_3_data = qt_loc_data.loc[qt_loc_data['location'] == 3].drop(['location'], axis=1)\n",
        "loc_4_data = qt_loc_data.loc[qt_loc_data['location'] == 4].drop(['location'], axis=1)\n",
        "loc_5_data = qt_loc_data.loc[qt_loc_data['location'] == 5].drop(['location'], axis=1)\n",
        "\n",
        "all_location_data = []\n",
        "all_location_data.append(loc_1_data)\n",
        "all_location_data.append(loc_2_data)\n",
        "all_location_data.append(loc_3_data)\n",
        "all_location_data.append(loc_4_data)\n",
        "all_location_data.append(loc_5_data)\n",
        "\n",
        "\n",
        "\n",
        "# split a univariate dataset into train/test sets\n",
        "def train_test_split(data, n_test):\n",
        "\treturn data[:-n_test], data[-n_test:]\n",
        "\n",
        "# transform list into supervised learning format\n",
        "def series_to_supervised(data, n_in, n_out=1):\n",
        "\tdf = DataFrame(data)\n",
        "\tcols = list()\n",
        "\t# input sequence (t-n, ... t-1)\n",
        "\tfor i in range(n_in, 0, -1):\n",
        "\t\tcols.append(df.shift(i))\n",
        "\t# forecast sequence (t, t+1, ... t+n)\n",
        "\tfor i in range(0, n_out):\n",
        "\t\tcols.append(df.shift(-i))\n",
        "\t# put it all together\n",
        "\tagg = concat(cols, axis=1)\n",
        "\t# drop rows with NaN values\n",
        "\tagg.dropna(inplace=True)\n",
        "\treturn agg.values\n",
        "\n",
        "# root mean squared error or rmse\n",
        "def measure_rmse(actual, predicted):\n",
        "\treturn sqrt(mean_squared_error(actual, predicted))\n",
        "\n",
        "# difference dataset\n",
        "def difference(data, order):\n",
        "\treturn [data[i] - data[i - order] for i in range(order, len(data))]\n",
        "\n",
        "# fit a model\n",
        "def model_fit(train, config, num_step):\n",
        "\t# unpack config\n",
        "\tn_input, n_nodes, n_epochs, n_batch, n_diff = config\n",
        "\t# prepare data\n",
        "\tif n_diff > 0:\n",
        "\t\ttrain = difference(train, n_diff)\n",
        "\t# transform series into supervised format\n",
        "\tdata = series_to_supervised(train, n_in=n_input, n_out=num_step)#####################\n",
        "\t# separate inputs and outputs\n",
        "\ttrain_x, train_y = data[:, :-num_step], data[:, -num_step:] ################\n",
        "\t# reshape input data into [samples, timesteps, features]\n",
        "\tn_features = 1 #if it is multivariate then this should be changed acccordingly\n",
        "\ttrain_x = train_x.reshape((train_x.shape[0], train_x.shape[1], n_features))\n",
        "\t# define model\n",
        "\tmodel = tf.keras.Sequential([\n",
        "\ttf.keras.layers.LSTM(n_nodes, activation='relu', input_shape=(n_input, n_features)),\n",
        "\ttf.keras.layers.Dense(n_nodes, activation='relu'),\n",
        "\ttf.keras.layers.Dense(num_step)])##################################\n",
        "\tmodel.compile(loss='mse', optimizer='adam')\n",
        "\t# fit model\n",
        "\tmodel.fit(train_x, train_y, epochs=n_epochs, batch_size=n_batch, verbose=0)\n",
        "\treturn model\n",
        "\n",
        "# forecast with the fit model\n",
        "def model_predict(model, history, config):\n",
        "\t# unpack config\n",
        "\tn_input, _, _, _, n_diff = config\n",
        "\t# prepare data\n",
        "\tcorrection = 0.0\n",
        "\tif n_diff > 0:\n",
        "\t\tcorrection = history[-n_diff]\n",
        "\t\thistory = difference(history, n_diff)\n",
        "\t# reshape sample into [samples, timesteps, features]\n",
        "\tx_input = array(history[-n_input:]).reshape((1, n_input, 1))\n",
        "\t# forecast\n",
        "\tyhat = model.predict(x_input, verbose=0)\n",
        "\treturn correction + yhat[0]\n",
        "\n",
        "# walk-forward validation for univariate data\n",
        "def walk_forward_validation(data, n_test, cfg, num_step):###########\n",
        "\terror_collection=list()################\n",
        "\t# split dataset\n",
        "\ttrain, test = train_test_split(data, n_test)\n",
        "\t# fit model\n",
        "\tmodel = model_fit(train, cfg, num_step)##############\n",
        "\t# seed history with training dataset\n",
        "\thistory = [x for x in train]\n",
        "\t# step over each time-step in the test set\n",
        "\tfor i in range(len(test)+1-num_step):############\n",
        "\t\t# fit model and make forecast for history\n",
        "\t\tyhat = model_predict(model, history, cfg)\n",
        "\t\terr = measure_rmse(test[i:i+num_step], yhat)#############\n",
        "\t\terror_collection.append(err)\n",
        "\t\t# store forecast in list of predictions\n",
        "\t\t# add actual observation to history for the next loop\n",
        "\t\thistory.append(test[i])\n",
        "\t# estimate prediction error\n",
        "\terror = mean(error_collection)######\n",
        "\t#print(' > %.3f' % error)\n",
        "\treturn error\n",
        "\n",
        "# score a model, return None on failure\n",
        "def repeat_evaluate(data, config, n_test, num_step, n_repeats=5):############\n",
        "\t# convert config to a key\n",
        "\tkey = str(config)\n",
        "\t# fit and evaluate the model n times\n",
        "\tscores = [walk_forward_validation(data, n_test, config, num_step) for _ in range(n_repeats)]##########\n",
        "\t# summarize score\n",
        "\tresult = mean(scores)\n",
        "\t\n",
        "\treturn (result, config)#######\n",
        "\n",
        "# grid search configs\n",
        "def grid_search(data, cfg_list, n_test, num_step):#########\n",
        "\t# evaluate configs\n",
        "\tscores = [repeat_evaluate(data, cfg, n_test, num_step) for cfg in cfg_list]#########\n",
        "\t# sort configs by error, asc\n",
        "\tscores.sort()##########\n",
        "\treturn scores\n",
        "\n",
        "# create a list of configs to try\n",
        "def model_configs():\n",
        "\t# define scope of configs\n",
        "\tn_input = [2,3,4,5]\n",
        "\tn_nodes = [20,30,40,50]\n",
        "\tn_epochs = [60]\n",
        "\tn_batch = [10]\n",
        "\tn_diff = [0]\n",
        "\t# create configs\n",
        "\tconfigs = list()\n",
        "\tfor i in n_input:\n",
        "\t\tfor j in n_nodes:\n",
        "\t\t\tfor k in n_epochs:\n",
        "\t\t\t\tfor l in n_batch:\n",
        "\t\t\t\t\tfor m in n_diff:\n",
        "\t\t\t\t\t\tcfg = [i, j, k, l, m]\n",
        "\t\t\t\t\t\tconfigs.append(cfg)\n",
        "\tprint('Total configs: %d' % len(configs))\n",
        "\treturn configs\n",
        "\n",
        "\n",
        "\n",
        "location_counter = 0\n",
        "n_test = 5 # keep this fixed\n",
        "location_data = all_location_data[location_counter]\n",
        "location_counter = location_counter + 1\n",
        "print('Location: %d' % location_counter)\n",
        "data = location_data.values[5:]\n",
        "step_list = []\n",
        "location_list = []\n",
        "cfg_list = []\n",
        "error_list = []\n",
        "window_length_list = []\n",
        "nodes_list = []\n",
        "epochs_list = []\n",
        "batch_size_list = []\n",
        "diff_list =[]\n",
        "\n",
        "for num in [3]:# number of steps to forecast, this can be any number between 1 and 5\n",
        "  num_step = num \n",
        "  cfg_list = model_configs()\n",
        "  scores = grid_search(data, cfg_list, n_test, num_step)\n",
        "  print('done for the step: %d' % num_step)\n",
        "  # list top 3 configs\n",
        "  error, cfg = scores[0]\n",
        "  print(cfg, error)\n",
        "  window_length_list.append(cfg[0])\n",
        "  nodes_list.append(cfg[1])\n",
        "  epochs_list.append(cfg[2])\n",
        "  batch_size_list.append(cfg[3])\n",
        "  diff_list.append(cfg[4])\n",
        "  error_list.append(error)\n",
        "  step_list.append(num_step)\n",
        "  location_list.append(location_counter)\n",
        "\n",
        "collection = DataFrame()\n",
        "collection['Location']  = location_list\n",
        "collection['Predict Step'] = step_list\n",
        "collection['Error'] = error_list\n",
        "collection['window width'] = window_length_list\n",
        "collection['Epochs'] = epochs_list\n",
        "collection['Nodes'] = nodes_list\n",
        "collection['batch size'] = batch_size_list\n",
        "collection['difference'] = diff_list\n",
        "collection.to_csv('TS.csv')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}